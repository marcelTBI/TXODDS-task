{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TXODDS task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "from queue import Queue\n",
    "from urllib import request\n",
    "import re\n",
    "import unittest\n",
    "\n",
    "# maximum number of queue items\n",
    "max_items = 10\n",
    "to_do = Queue(max_items)\n",
    "\n",
    "#write ulrs.txt file, so this is self-contained example:\n",
    "with open(\"urls.txt\", \"w\") as f:\n",
    "    print(\"http://example.com\", file=f)\n",
    "    print(\"google.com\", file=f)\n",
    "    print(\"http://www.gcgedgtsdgasdl.com/\", file=f)\n",
    "    print(\"http://www.tbi.univie.ac.at/~marcel\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_url(line):\n",
    "    # try to fetch the designated url:\n",
    "    if len(line)==0:\n",
    "        return None\n",
    "    print(\"Try to fetch: \", line)\n",
    "    try:\n",
    "        opened_url = request.urlopen(line, timeout=2)\n",
    "    except request.URLError as err:\n",
    "        print(\"Error on fetching:\", line, err, file=sys.stderr)\n",
    "    except ValueError as err:\n",
    "        print(\"Unknown url type, forgot \\\"http(s)://\\\"?\", line, err, file=sys.stderr)\n",
    "    else:\n",
    "        return opened_url\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Producer(Thread):\n",
    "    def run(self):\n",
    "        global to_do\n",
    "        print(\"Producer starts\")\n",
    "        # open the designated stream/file (in my case urls.txt)\n",
    "        with open(\"urls.txt\", \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                opened_url = fetch_url(line)\n",
    "                if opened_url:\n",
    "                    to_do.put((line, opened_url))\n",
    "                    print(\"Fetched:\", line, file=sys.stdout)\n",
    "                \n",
    "        print(\"Producer stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_hyperlinks(html):\n",
    "    # hyperlinks regexp, can be done much more sophistically\n",
    "    hlink_reg = re.compile(\"http://[^ >\\\"\\'\\\\\\\\]+|https://[^ >\\\"\\'\\\\\\\\]+\")\n",
    "    hlinks = hlink_reg.findall(html)\n",
    "    return hlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Consumer(Thread):\n",
    "    \n",
    "    stopping = False\n",
    "    \n",
    "    def stop(self):\n",
    "        print(\"Consumer stops\")\n",
    "        self.stopping = True\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Consumer starts\")\n",
    "        global to_do\n",
    "        \n",
    "        # delete contents of output file\n",
    "        with open(\"output.txt\", \"w\"): pass\n",
    "        \n",
    "        # consumer loop\n",
    "        while not self.stopping:\n",
    "            # fetch the webpage\n",
    "            url_adress, opened_url = to_do.get()\n",
    "            to_do.task_done()\n",
    "            print(\"Crawling:\", opened_url, file=sys.stdout)\n",
    "            \n",
    "            # parse the page:\n",
    "            hyperlinks = extract_hyperlinks(str(opened_url.read()))\n",
    "            \n",
    "            # write them in the output:\n",
    "            with open(\"output.txt\", \"a\") as f_out:\n",
    "                print(\"Opened: \", url_adress, \"\\nRedirected to:\", opened_url.geturl().strip(), file=f_out)\n",
    "                for i, hyper in enumerate(hyperlinks):\n",
    "                    print(i+1, hyper, file=f_out)\n",
    "                print(\"\", file=f_out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer starts\n",
      "Try to fetch:  http://example.com\n",
      "Consumer starts\n",
      "Fetched: http://example.com\n",
      "Try to fetch:  google.com\n",
      "Try to fetch:  http://www.gcgedgtsdgasdl.com/\n",
      "Crawling: <http.client.HTTPResponse object at 0x0000000005559DD8>\n",
      "Try to fetch:  http://www.tbi.univie.ac.at/~marcel\n",
      "Fetched: http://www.tbi.univie.ac.at/~marcel\n",
      "Producer stops\n",
      "Crawling: <http.client.HTTPResponse object at 0x0000000005559FD0>\n",
      "Consumer stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown url type, forgot \"http(s)://\"? google.com unknown url type: 'google.com'\n",
      "Error on fetching: http://www.gcgedgtsdgasdl.com/ <urlopen error [Errno 11004] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "Producer().start()\n",
    "consumer = Consumer()\n",
    "consumer.start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "consumer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Consumer (__main__.Test) ... ok\n",
      "test_Producer (__main__.Test) ... ok\n",
      "test_extract_hyoerlinks (__main__.Test) ... ok\n",
      "test_fetch_url (__main__.Test) ... Unknown url type, forgot \"http(s)://\"? google.com unknown url type: 'google.com'\n",
      "ok"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to fetch:  google.com\n",
      "Try to fetch:  https://google.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.303s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test(unittest.TestCase):\n",
    "    def test_extract_hyoerlinks(self):\n",
    "        hlinks = extract_hyperlinks(\"http://a.com <a \\\"https://b.com\\\"> \")\n",
    "        self.assertEqual(len(hlinks), 2)\n",
    "        self.assertEqual(hlinks[0], \"http://a.com\")\n",
    "        self.assertEqual(hlinks[1], \"https://b.com\")\n",
    "    def test_fetch_url(self):\n",
    "        self.assertEqual(None, fetch_url(\"\"))\n",
    "        self.assertEqual(None, fetch_url(\"google.com\"))\n",
    "        # assume google is always available\n",
    "        self.assertTrue(fetch_url(\"https://google.com\")) \n",
    "        pass\n",
    "    def test_Producer(self):\n",
    "        pass\n",
    "    def test_Consumer(self):\n",
    "        pass\n",
    "    \n",
    "unittests = unittest.TestLoader().loadTestsFromTestCase(Test)\n",
    "unittest.TextTestRunner(verbosity=2).run(unittests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
